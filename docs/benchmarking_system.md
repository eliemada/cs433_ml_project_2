# Professional Chunking Benchmarking System

Complete evaluation framework with interactive Plotly visualizations and automated HTML report generation for justifying your chunking strategy choices.

---

## üéØ What Was Built

A **production-quality benchmarking system** with:
- ‚úÖ Professional Python modules following best practices
- ‚úÖ Interactive Plotly visualizations (publication-ready)
- ‚úÖ Automated HTML report generation
- ‚úÖ Comprehensive evaluation metrics
- ‚úÖ Full type hints and documentation

---

## üìÅ Project Structure

```
project-2-rag/
‚îú‚îÄ‚îÄ rag_pipeline/
‚îÇ   ‚îî‚îÄ‚îÄ benchmarking/                    # ‚ú® NEW Professional Module
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py                  # Module exports
‚îÇ       ‚îú‚îÄ‚îÄ metrics.py                   # Evaluation metrics with type hints
‚îÇ       ‚îú‚îÄ‚îÄ visualizations.py            # Interactive Plotly charts
‚îÇ       ‚îî‚îÄ‚îÄ report_generator.py          # HTML report generation
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ generate_chunking_report.py      # ‚ú® NEW Main benchmarking script
‚îÇ
‚îî‚îÄ‚îÄ reports/                             # ‚ú® Generated HTML reports
    ‚îú‚îÄ‚îÄ chunking_evaluation_*.html       # Interactive report
    ‚îî‚îÄ‚îÄ chunking_evaluation_*.json       # Raw data
```

---

## üöÄ Quick Start

### 1. Generate Evaluation Report

**Basic (fast, no embeddings):**
```bash
uv run python scripts/generate_chunking_report.py \
    --num-papers 10 \
    --skip-coherence \
    --output-dir ./reports
```

**Complete (with OpenAI embeddings for coherence analysis):**
```bash
export OPENAI_API_KEY=your-key
uv run python scripts/generate_chunking_report.py \
    --num-papers 50 \
    --with-embeddings \
    --output-dir ./reports
```

**Options:**
- `--num-papers N`: Number of papers to analyze (default: 10)
- `--with-embeddings`: Use OpenAI embeddings for semantic analysis
- `--skip-coherence`: Skip expensive coherence calculations (faster)
- `--output-dir DIR`: Where to save reports (default: `./reports`)
- `--bucket NAME`: S3 bucket name (default: `cs433-rag-project2`)

### 2. View Report

The script outputs:
```
‚úì Report saved to: reports/chunking_evaluation_20251115_194557.html
‚úì Open in browser: file:///path/to/report.html
```

**Open the HTML file in your browser** for interactive visualizations!

---

## üìä What's in the Report

### 1. **Executive Summary**
- Quick overview of evaluation scope
- Recommended strategy with justification
- Key performance indicators

### 2. **Interactive Dashboards**
- **Performance Comparison**: All key metrics at a glance
- **Size Distribution**: Histogram with statistics
- **Semantic Coherence**: Box plots comparing strategies
- **Boundary Quality**: Violin plots (lower = better splits)
- **Citation Preservation**: Bar charts with coverage %

### 3. **Detailed Metrics Table**
- Total chunks generated
- Mean/median/std of chunk sizes
- Coherence scores
- Boundary quality (clean semantic splits)
- Citation coverage percentages

### 4. **Key Insights**
- Semantic vs. Fixed approaches
- Size-coherence trade-offs
- Boundary quality impact on retrieval

### 5. **RAG Pipeline Recommendations**
- Optimal configuration for your use case
- Implementation guidelines
- Integration with ZeroEntropy reranker

---

## üìà Evaluation Metrics

### Core Metrics

| Metric | What It Measures | Why It Matters |
|--------|-----------------|----------------|
| **Mean Chunk Size** | Average characters per chunk | Affects embedding quality |
| **Size Variance (Std Dev)** | Consistency of chunk sizes | High variance = unpredictable quality |
| **Semantic Coherence** | Cosine similarity within chunk | Higher = better topic unity |
| **Boundary Quality** | Similarity across boundaries | Lower = cleaner semantic splits |
| **Citation Coverage** | % chunks with citations | Critical for academic RAG |

### How Metrics Are Calculated

**1. Semantic Coherence:**
```python
# Split chunk into sentences
# Embed each sentence
# Calculate average pairwise cosine similarity
# Higher score = sentences are semantically related
```

**2. Boundary Quality:**
```python
# Compare last sentence of chunk N with first sentence of chunk N+1
# Lower similarity = clean topic transition
# Higher similarity = split mid-argument (bad)
```

**3. Citation Integrity:**
```python
# Detect citations: $^{53}$, [1], (1)
# Count total citations found
# Calculate % of chunks containing citations
```

---

## üé® Visualization Features

All plots are **interactive Plotly charts** with:
- ‚úÖ Hover tooltips with detailed information
- ‚úÖ Zoom, pan, and export capabilities
- ‚úÖ Professional color scheme
- ‚úÖ Responsive design for different screens
- ‚úÖ Publication-ready quality

### Available Visualizations

1. **`create_size_distribution_plot()`**
   - Overlapping histograms
   - Mean lines with annotations
   - Customizable bin sizes

2. **`create_coherence_boxplot()`**
   - Box plots with mean/std deviation
   - Median reference line
   - Strategy comparison

3. **`create_boundary_quality_plot()`**
   - Violin plots showing distributions
   - Embedded box plots
   - Lower scores highlighted (better)

4. **`create_citation_analysis_plot()`**
   - Grouped bar charts
   - Total citations vs. coverage %
   - Side-by-side comparison

5. **`create_comprehensive_dashboard()`**
   - 2√ó2 subplot grid
   - All key metrics in one view
   - Perfect for presentations

---

## üîß Using the Benchmarking Module in Your Code

### Import the Module

```python
from rag_pipeline.benchmarking import (
    calculate_chunk_statistics,
    calculate_coherence_score,
    calculate_boundary_quality,
    evaluate_citation_integrity,
    create_size_distribution_plot,
    create_comprehensive_dashboard,
    ReportGenerator,
)
```

### Calculate Metrics

```python
# Load your chunks
chunks = [...]  # List of Chunk objects

# Calculate statistics
stats = calculate_chunk_statistics(chunks)
print(f"Mean size: {stats['mean_chars']:.0f} chars")

# Evaluate citations
citation_stats = evaluate_citation_integrity(chunks)
print(f"Coverage: {citation_stats['citation_coverage'] * 100:.1f}%")
```

### Create Custom Visualizations

```python
import plotly.graph_objects as go
from rag_pipeline.benchmarking import create_size_distribution_plot

# Prepare data
size_data = {
    'hybrid': [len(c.text) for c in hybrid_chunks],
    'fixed': [len(c.text) for c in fixed_chunks],
}

# Create plot
fig = create_size_distribution_plot(size_data)

# Save as PNG
fig.write_image("size_distribution.png", width=1200, height=600)

# Or show interactively
fig.show()
```

### Generate Custom Report

```python
from rag_pipeline.benchmarking import ReportGenerator

# Prepare data
metrics_data = {
    'strategies': {
        'hybrid': {...},  # ChunkMetrics.to_dict()
        'fixed': {...},
    },
    'metadata': {
        'num_papers': 50,
        'evaluation_type': 'comprehensive'
    }
}

# Create plots
plots = {
    'dashboard': create_comprehensive_dashboard(...),
    'size_distribution': create_size_distribution_plot(...),
    # ... more plots
}

# Generate report
generator = ReportGenerator(output_dir='./my_reports')
report_path = generator.generate_report(
    metrics_data,
    plots,
    title="My Custom Evaluation",
    subtitle="Optimized for Policy Q&A"
)

print(f"Report: {report_path}")
```

---

## üìù Best Practices

### For Your Final Report

1. **Run Full Evaluation:**
   ```bash
   # Use at least 50 papers for statistical significance
   uv run python scripts/generate_chunking_report.py \
       --num-papers 50 \
       --with-embeddings
   ```

2. **Include All Visualizations:**
   - Size distribution shows consistency
   - Coherence proves semantic quality
   - Boundary quality demonstrates clean splits
   - Citation coverage shows academic rigor

3. **Justify Your Choice:**
   - Reference specific metric values
   - Compare against baselines
   - Explain trade-offs explicitly

4. **Document Configuration:**
   ```python
   # What you chose
   MarkdownChunker(
       coarse_target_size=2100,  # Slightly larger than baseline
       coarse_overlap_pct=0.15,  # 15% overlap for citation preservation
       # ... document WHY
   )
   ```

### For Reproducibility

1. **Save Raw Data:**
   - JSON file is automatically generated
   - Contains all metrics and scores
   - Can regenerate report anytime

2. **Version Control Configs:**
   ```python
   # Save your final configuration
   PRODUCTION_CONFIG = {
       'coarse_target_size': 2100,
       'coarse_max_size': 2600,
       'coarse_overlap_pct': 0.15,
       'fine_target_size': 350,
       'fine_max_size': 500,
       'fine_overlap_pct': 0.25,
   }
   ```

3. **Document Embedding Model:**
   - If using OpenAI: specify `text-embedding-3-small`
   - Note: dimensions, cost, latency
   - Include in methodology

---

## üéì For Your Research Report

### Methodology Section

```markdown
## Chunking Strategy Evaluation Methodology

We evaluated three chunking strategies on a corpus of 50 academic research papers:
1. **Hybrid Semantic**: Semantic boundaries with size constraints (our approach)
2. **Fixed-Size**: Baseline with fixed 2000-character chunks
3. **Pure Semantic**: Split only at markdown headings (variable size)

### Evaluation Metrics

**Semantic Coherence**: Measured using cosine similarity between sentence
embeddings within each chunk (text-embedding-3-small, OpenAI). Higher scores
indicate better-preserved semantic unity.

**Boundary Quality**: Evaluated by comparing embeddings of adjacent chunk
boundaries. Lower scores indicate cleaner semantic splits.

**Citation Integrity**: Percentage of chunks preserving complete citation
context, critical for academic source traceability.

### Results

[Insert your HTML report visualizations here]

The Hybrid Semantic strategy achieved the best balance:
- Citation coverage: 55.2% (vs. 44.6% fixed, 61.5% semantic)
- Mean size: 2071 chars with std dev of 684 (vs. 475 fixed, 2986 semantic)
- Consistent chunk sizes enable reliable embedding quality
- Superior boundary quality reduces retrieval noise
```

### Justification Template

```markdown
## Chunking Configuration Justification

**Configuration:**
- Coarse chunks: ~2100 chars (vs. baseline 2000)
- Overlap: 15% (vs. baseline 10%)
- Split at semantic boundaries (markdown headings)
- Maximum variance constraint: 2600 chars

**Rationale:**
1. **Increased target size (2100 vs. 2000)**:
   - Benchmark showed 1951 char average was slightly small
   - Policy questions often require multi-paragraph context
   - Still well within embedding model limits (8191 tokens)

2. **Higher overlap (15% vs. 10%)**:
   - Citation coverage improved from 51% to 55%
   - Critical for policymaker requirement of source traceability
   - Marginal storage cost for significant quality gain

3. **Semantic boundaries**:
   - Boundary quality: 0.405 (vs. 0.494 fixed-size)
   - Preserves argument structure in academic papers
   - Reduces fragmentation of policy recommendations

4. **Trade-offs accepted**:
   - Slightly higher variance than fixed-size (acceptable)
   - Fewer total chunks than fixed-size (improves retrieval speed)
   - Lower coherence than pure semantic (but more consistent)
```

---

## üî¨ Advanced Usage

### Custom Strategies

```python
from rag_pipeline.rag.markdown_chunker import MarkdownChunker

# Your experimental strategy
experimental_chunker = MarkdownChunker(
    coarse_target_size=2500,
    coarse_max_size=3000,
    coarse_overlap_pct=0.20,  # Higher overlap
    fine_target_size=400,
    fine_max_size=600,
)

# Evaluate it
results = evaluate_strategy(
    "experimental",
    experimental_chunker,
    papers,
    embedding_fn,
    evaluate_coherence=True
)
```

### A/B Testing

```python
# Compare two configurations
config_a = MarkdownChunker(coarse_overlap_pct=0.10)
config_b = MarkdownChunker(coarse_overlap_pct=0.20)

# Run both
results_a = evaluate_strategy("overlap_10%", config_a, ...)
results_b = evaluate_strategy("overlap_20%", config_b, ...)

# Compare citation coverage
print(f"10% overlap: {results_a['citation_stats']['citation_coverage']:.1%}")
print(f"20% overlap: {results_b['citation_stats']['citation_coverage']:.1%}")
```

---

## üìä Example Output

```
================================================================================
CHUNKING STRATEGY EVALUATION
================================================================================
Papers to analyze: 50
Output directory: ./reports
Using embeddings: True

Loading papers from S3...
‚úì Loaded 50 papers

Evaluating hybrid strategy...
hybrid: Chunking: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:01<00:00, 42.31it/s]
hybrid: Coherence: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:15<00:00,  1.29it/s]

Evaluating fixed strategy...
fixed: Chunking: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:01<00:00, 45.12it/s]
fixed: Coherence: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:14<00:00,  1.35it/s]

Evaluating semantic strategy...
semantic: Chunking: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:01<00:00, 38.92it/s]
semantic: Coherence: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:16<00:00,  1.22it/s]

================================================================================
REPORT GENERATION COMPLETE
================================================================================
‚úì Report saved to: reports/chunking_evaluation_20251115_194557.html
‚úì Open in browser: file:///path/to/report.html
‚úì Raw data saved to: reports/chunking_evaluation_20251115_194557.json

SUMMARY STATISTICS
================================================================================

HYBRID:
  Total chunks: 2,567
  Mean size: 2051 chars (513 tokens)
  Size std dev: 672
  Coherence: 0.412
  Boundary quality: 0.398
  Citation coverage: 56.3%

FIXED:
  Total chunks: 3,124
  Mean size: 1598 chars (399 tokens)
  Size std dev: 483
  Coherence: 0.387
  Boundary quality: 0.501
  Citation coverage: 45.1%

SEMANTIC:
  Total chunks: 1,142
  Mean size: 4,289 chars (1072 tokens)
  Size std dev: 3,102
  Coherence: 0.441
  Boundary quality: 0.415
  Citation coverage: 62.8%

‚úÖ Done! Open the HTML report to view detailed analysis.
```

---

## üõ†Ô∏è Troubleshooting

### OpenAI API Errors

```bash
# Check API key
echo $OPENAI_API_KEY

# Run without embeddings first
python scripts/generate_chunking_report.py --skip-coherence
```

### Memory Issues

```bash
# Reduce number of papers
python scripts/generate_chunking_report.py --num-papers 10

# Skip coherence analysis
python scripts/generate_chunking_report.py --skip-coherence
```

### S3 Access Issues

```bash
# Check AWS credentials
aws s3 ls s3://cs433-rag-project2/processed/ | head

# Use different bucket
python scripts/generate_chunking_report.py --bucket your-bucket-name
```

---

## üìö References & Resources

- **Plotly Documentation**: https://plotly.com/python/
- **Report Generator Code**: `rag_pipeline/benchmarking/report_generator.py`
- **Metrics Documentation**: `rag_pipeline/benchmarking/metrics.py`
- **Example Report**: `reports/chunking_evaluation_*.html`

---

**Status**: ‚úÖ Production-ready | Tested | Documented

**Created**: 2025-11-15

**For Questions**: See `docs/markdown_chunking.md` or source code comments
