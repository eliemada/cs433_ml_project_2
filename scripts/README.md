# Distributed PDF Processing Scripts

## Overview

This directory contains scripts for distributed parallel processing of PDFs using AWS EC2 Spot instances.

## Your S3 Structure

```
s3://cs433-rag-project2/
â”œâ”€â”€ raw_pdfs/                              # Input PDFs (4000 files)
â”‚   â”œâ”€â”€ 00002_W2122361802_Navigating...pdf
â”‚   â”œâ”€â”€ 00003_W2122361803_...pdf
â”‚   â””â”€â”€ ...
â”œâ”€â”€ processed/                             # Output markdown files
â”‚   â”œâ”€â”€ 00002_W2122361802_Navigating...md
â”‚   â””â”€â”€ ...
â”œâ”€â”€ raw_metadata/                          # Existing metadata
â””â”€â”€ failures/                              # Failure reports (auto-created)
    â”œâ”€â”€ worker-0-failures.json
    â””â”€â”€ ...
```

## Files

- **`distributed_worker.py`** - Main worker script that runs on each EC2 instance
- **`utils/s3_utils.py`** - S3 helper functions (list, download, upload, exists)
- **`utils/worker_distribution.py`** - Work partitioning logic
- **`.env.example`** - Example configuration file

## Quick Start

### 1. Set Up Environment Variables

Copy the example and configure:

```bash
cp .env.example .env
# Edit .env with your settings
```

**Required variables:**
```bash
WORKER_ID=0                              # Unique ID: 0, 1, 2, 3, 4
TOTAL_WORKERS=5                          # Total parallel workers
S3_INPUT_BUCKET=cs433-rag-project2       # Your bucket
S3_INPUT_PREFIX=raw_pdfs/                # Input folder
S3_OUTPUT_BUCKET=cs433-rag-project2      # Output bucket
S3_OUTPUT_PREFIX=processed/              # Output folder
```

### 2. Test Locally (Optional)

Test with a single PDF before launching to AWS:

```bash
# Set environment variables
export WORKER_ID=0
export TOTAL_WORKERS=1
export S3_INPUT_BUCKET=cs433-rag-project2
export S3_INPUT_PREFIX=raw_pdfs/
export S3_OUTPUT_BUCKET=cs433-rag-project2
export S3_OUTPUT_PREFIX=processed/

# Run worker
python distributed_worker.py
```

This will process ALL PDFs since TOTAL_WORKERS=1. For testing, you might want to manually filter in the code first.

### 3. Launch on AWS

See the main design document: `docs/plans/2025-11-09-distributed-pdf-processing-design.md`

## How Work Distribution Works

With 5 workers and 10 PDFs:

```python
# PDFs are sorted alphabetically first
all_pdfs = [
    'raw_pdfs/00002_W2122361802_...pdf',  # index 0 â†’ Worker 0
    'raw_pdfs/00003_W2122361803_...pdf',  # index 1 â†’ Worker 1
    'raw_pdfs/00004_W2122361804_...pdf',  # index 2 â†’ Worker 2
    'raw_pdfs/00005_W2122361805_...pdf',  # index 3 â†’ Worker 3
    'raw_pdfs/00006_W2122361806_...pdf',  # index 4 â†’ Worker 4
    'raw_pdfs/00007_W2122361807_...pdf',  # index 5 â†’ Worker 0
    'raw_pdfs/00008_W2122361808_...pdf',  # index 6 â†’ Worker 1
    'raw_pdfs/00009_W2122361809_...pdf',  # index 7 â†’ Worker 2
    'raw_pdfs/00010_W2122361810_...pdf',  # index 8 â†’ Worker 3
    'raw_pdfs/00011_W2122361811_...pdf',  # index 9 â†’ Worker 4
]

# Each worker processes: if (index % TOTAL_WORKERS == WORKER_ID)
```

**Result**: Each worker gets ~800 PDFs, no overlap, no coordination needed!

## Idempotent Processing

Workers automatically skip PDFs that have already been processed:

```python
# Before processing raw_pdfs/00002_W2122361802_...pdf
# Check if processed/00002_W2122361802_...md exists
# If yes â†’ skip
# If no â†’ process
```

This means you can:
- Re-run workers safely
- Resume after failures
- Add more workers mid-run

## Monitoring

### Check Progress

Count processed files:
```bash
aws s3 ls s3://cs433-rag-project2/processed/ --recursive | wc -l
```

### View Failures

Check failure reports:
```bash
aws s3 cp s3://cs433-rag-project2/failures/worker-0-failures.json -
```

### CloudWatch Logs

When running on EC2, logs go to CloudWatch Logs:
- Log group: `/aws/ec2/pdf-workers/`
- Log streams: `worker-0`, `worker-1`, etc.

## Output Format

Input PDF:
```
raw_pdfs/00002_W2122361802_Navigating_the_Patent_Thicket_Cross_Licenses,_Patent_Pools,_and_Standard-Setting.pdf
```

Output markdown:
```
processed/00002_W2122361802_Navigating_the_Patent_Thicket_Cross_Licenses,_Patent_Pools,_and_Standard-Setting.md
```

Same filename, just `.pdf` â†’ `.md` and different folder.

## Cost Estimate

With 5 workers on g4dn.xlarge Spot instances:
- **Cost per hour**: 5 Ã— $0.158 = $0.79/hour
- **Time to process 4000 PDFs**: ~33 hours (at 2.5 min/PDF)
- **Total cost**: ~$26

Well under your $100 budget! ðŸ’°

## Troubleshooting

**Worker doesn't find PDFs**
- Check `S3_INPUT_BUCKET` and `S3_INPUT_PREFIX` are correct
- Verify AWS credentials have S3 read permissions

**Output not uploading**
- Check AWS credentials have S3 write permissions
- Verify `S3_OUTPUT_BUCKET` exists

**Dolphin model errors**
- Check GPU is available: `nvidia-smi`
- Verify model weights are downloaded (should be in Docker image)

## Next Steps

1. âœ… **Phase 1 Complete**: Worker script and utilities
2. ðŸš§ **Phase 2**: Update Docker image
3. ðŸ“‹ **Phase 3**: Create AWS launch script
4. ðŸš€ **Phase 4**: Launch and monitor

See design document for full implementation plan.
